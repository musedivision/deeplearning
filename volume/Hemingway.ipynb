{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dill as pickle\n",
    "\n",
    "#fastai\n",
    "from fastai.imports import *\n",
    "from fastai.torch_imports import *\n",
    "from fastai.core import *\n",
    "from fastai.model import fit\n",
    "from fastai.dataset import *\n",
    "\n",
    "import torchtext\n",
    "from torchtext import vocab, data\n",
    "from torchtext.datasets import language_modeling\n",
    "\n",
    "from fastai.rnn_reg import * \n",
    "from fastai.rnn_train import *\n",
    "from fastai.nlp import *\n",
    "from fastai.lm_rnn import *\n",
    "\n",
    "#pytorch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable as V\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_tok = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set theme \n",
    "# from jupyterthemes.stylefx import set_nb_theme\n",
    "# set_nb_theme('monokai')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH=Path('data/hemingway')\n",
    "PATH.mkdir(exist_ok=True)\n",
    "Path(PATH/'train').mkdir(exist_ok=True)\n",
    "Path(PATH/'val').mkdir(exist_ok=True)\n",
    "Path(PATH/'test').mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/hemingway'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save full hemingway with code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(Path('/notebooks/volume/full_hemingway.txt'), encoding='utf-8')\n",
    "fulltext = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s blind, unforgivingly jealous of what had happened to him. The fact\\nthat I took it as a matter of course did not alter that any. I certainly\\ndid hate him. I do not think I ever really hated him until he had that\\nlittle spell of superiority at lunch—that and when he went through all\\nthat barbering. '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = np.random.randint(1,len(fulltext))\n",
    "fulltext[r:(r+300)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_tokens = spacy_tok(fulltext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split hemingway by simply by line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lng = len(fulltext)\n",
    "trn,val = [fulltext[0:int(lng*i)] for i in [0.8,0.2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to files\n",
    "def save_txt_file(name, txt):    \n",
    "    f=open(PATH/name, mode='w', encoding='utf-8')\n",
    "    f.write(txt)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_txt_file('hemingway.trn.txt', trn)\n",
    "save_txt_file('hemingway.val.txt', val)\n",
    "# save_txt_file('hemingway.test.txt', test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2195"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(TEXT.vocab.itos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs, bptt = 80, 70\n",
    "TEXT = data.Field(lower=True, tokenize='spacy')\n",
    "FILES = dict(train='hemingway.trn.txt', val='hemingway.val.txt', test='hemingway.text.txt')\n",
    "md = LanguageModelData.from_text_files(PATH, TEXT, 'hemingway.trn.txt', 'hemingway.val.txt', 'hemingway.test.txt', bs=bs, bptt=bptt, min_freq=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(TEXT, open(f'{PATH}/models/TEXT.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "em_sz = 200  # size of each embedding vector\n",
    "nh = 500     # number of hidden activations per layer\n",
    "nl = 3       # number of layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_fn = partial(optim.Adam, betas=(0.7, 0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = md.get_model(opt_fn, em_sz, nh, nl,\n",
    "                           dropouti=0.05, dropout=0.05, wdrop=0.1, dropoute=0.02, dropouth=0.05)\n",
    "learner.reg_fn = seq2seq_reg\n",
    "learner.clip=0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequentialRNN(\n",
       "  (0): RNN_Encoder(\n",
       "    (encoder): Embedding(2195, 200, padding_idx=1)\n",
       "    (encoder_with_dropout): EmbeddingDropout(\n",
       "      (embed): Embedding(2195, 200, padding_idx=1)\n",
       "    )\n",
       "    (rnns): ModuleList(\n",
       "      (0): WeightDrop(\n",
       "        (module): LSTM(200, 500, dropout=0.05)\n",
       "      )\n",
       "      (1): WeightDrop(\n",
       "        (module): LSTM(500, 500, dropout=0.05)\n",
       "      )\n",
       "      (2): WeightDrop(\n",
       "        (module): LSTM(500, 200, dropout=0.05)\n",
       "      )\n",
       "    )\n",
       "    (dropouti): LockedDropout(\n",
       "    )\n",
       "    (dropouths): ModuleList(\n",
       "      (0): LockedDropout(\n",
       "      )\n",
       "      (1): LockedDropout(\n",
       "      )\n",
       "      (2): LockedDropout(\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): LinearDecoder(\n",
       "    (decoder): Linear(in_features=200, out_features=2195, bias=False)\n",
       "    (dropout): LockedDropout(\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2aac57eedb1f42b992909e2cb5f66fe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=15), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 26/59 [00:04<00:05,  5.84it/s, loss=5.82]\n",
      " 46%|████▌     | 27/59 [00:04<00:05,  5.86it/s, loss=5.8] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-54:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/fastai/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/envs/fastai/lib/python3.6/site-packages/tqdm/_monitor.py\", line 62, in run\n",
      "    for instance in self.tqdm_cls._instances:\n",
      "  File \"/opt/conda/envs/fastai/lib/python3.6/_weakrefset.py\", line 60, in __iter__\n",
      "    for itemref in self.data:\n",
      "RuntimeError: Set changed size during iteration\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                            \n",
      "    0      5.51389    5.356874  \n",
      "    1      5.4123     5.365284                            \n",
      "    2      5.379502   5.350643                            \n",
      "    3      5.378961   5.370271                            \n",
      "    4      5.37285    5.357415                            \n",
      "    5      5.365857   5.353315                            \n",
      "    6      5.352339   5.33947                             \n",
      "    7      5.224696   4.836269                            \n",
      "    8      4.751812   4.243293                            \n",
      "    9      4.356316   3.974646                            \n",
      "    10     4.102002   3.843693                            \n",
      "    11     3.955219   3.763725                            \n",
      "    12     3.858894   3.701036                            \n",
      "    13     3.810454   3.684139                            \n",
      "    14     3.783921   3.682809                            \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3.682809]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.fit(3e-3, 4, wds=1e-6, cycle_len=1, cycle_mult=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f6378e0c50c45e3a0a8c19a73c28df5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                            \n",
      "    0      3.812834   3.665374  \n",
      "    1      3.776463   3.605549                            \n",
      "    2      3.706881   3.559662                            \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3.5596619]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.fit(3e-3, 2, wds=1e-6, cycle_len=1, cycle_mult=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fdb272397e046ac8863d802dd7fcd86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                            \n",
      "    0      3.710703   3.531252  \n",
      "    1      3.644434   3.490881                            \n",
      "    2      3.644039   3.47948                             \n",
      "    3      3.593937   3.409544                            \n",
      "    4      3.532881   3.354022                            \n",
      "    5      3.481076   3.336434                            \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3.3364341]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.fit(3e-3, 2, wds=1e-6, cycle_len=2, cycle_mult=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c47a3d642feb456da1f7ec6092dda06d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=7), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                            \n",
      "    0      3.452714   3.334925  \n",
      "    1      3.449813   3.312935                            \n",
      "    2      3.438204   3.29424                             \n",
      "    3      3.442936   3.28781                             \n",
      "    4      3.415574   3.261163                            \n",
      "    5      3.39094    3.230272                            \n",
      "    6      3.367583   3.227625                            \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3.227625]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.fit(1e-3, 3, wds=1e-6, cycle_len=1, cycle_mult=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "082d82f114c549ea9e953cb2edbecdfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=2046), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                            \n",
      "    0      3.220329   3.005607  \n",
      "    1      3.167204   2.972532                            \n",
      "    2      3.174891   2.972322                            \n",
      "    3      3.139277   2.936433                            \n",
      "    4      3.109345   2.908558                            \n",
      "    5      3.090588   2.90291                             \n",
      "    6      3.120181   2.914815                            \n",
      "    7      3.093592   2.884377                            \n",
      "    8      3.084477   2.858088                            \n",
      "    9      3.050072   2.827158                            \n",
      "    10     3.013362   2.802395                            \n",
      "    11     2.988102   2.790097                            \n",
      "    12     2.977916   2.777191                            \n",
      "    13     2.977269   2.771533                            \n",
      "    14     3.006637   2.799065                            \n",
      "    15     2.995758   2.775349                            \n",
      "    16     2.971883   2.740152                            \n",
      "    17     2.973795   2.734345                            \n",
      "    18     2.934563   2.686271                            \n",
      "    19     2.892242   2.655482                            \n",
      "    20     2.889724   2.635882                            \n",
      "    21     2.868074   2.606966                            \n",
      "    22     2.827473   2.582138                            \n",
      "    23     2.810201   2.566114                            \n",
      "    24     2.792336   2.552262                            \n",
      "    25     2.80191    2.537148                            \n",
      "    26     2.786866   2.528425                            \n",
      "    27     2.764275   2.522492                            \n",
      "    28     2.774757   2.520804                            \n",
      "    29     2.774471   2.518958                            \n",
      "    30     2.824513   2.568061                            \n",
      "    31     2.812624   2.535758                            \n",
      "    32     2.789012   2.509001                            \n",
      "    33     2.772571   2.490084                            \n",
      "    34     2.763023   2.470045                            \n",
      "    35     2.721624   2.440279                            \n",
      "    36     2.697941   2.406879                            \n",
      "    37     2.673576   2.382263                            \n",
      "    38     2.646763   2.355908                            \n",
      "    39     2.618987   2.329372                            \n",
      "    40     2.622249   2.317483                            \n",
      "    41     2.609318   2.293329                            \n",
      "    42     2.588873   2.275127                            \n",
      "    43     2.56271    2.253686                            \n",
      "    44     2.543761   2.223891                            \n",
      "    45     2.518031   2.207181                            \n",
      "    46     2.502764   2.1893                              \n",
      "    47     2.476341   2.167705                            \n",
      "    48     2.475559   2.181182                            \n",
      "    49     2.506833   2.146821                            \n",
      "    50     2.472564   2.130626                            \n",
      "    51     2.459418   2.12772                             \n",
      "    52     2.462574   2.107075                            \n",
      "    53     2.445573   2.098013                            \n",
      "    54     2.4245     2.091905                            \n",
      "    55     2.401781   2.085996                            \n",
      "    56     2.443086   2.082225                            \n",
      "    57     2.429363   2.078803                            \n",
      "    58     2.408522   2.073695                            \n",
      "    59     2.380023   2.07012                             \n",
      "    60     2.423657   2.068648                            \n",
      "    61     2.423757   2.073492                            \n",
      "    62     2.466088   2.164795                            \n",
      "    63     2.498636   2.134752                            \n",
      "    64     2.45974    2.104827                            \n",
      "    65     2.515824   2.115375                            \n",
      "    66     2.46227    2.078625                            \n",
      "    67     2.437289   2.052473                            \n",
      "    68     2.389756   2.020985                            \n",
      "    69     2.384303   2.005999                            \n",
      "    70     2.363416   2.007629                            \n",
      "    71     2.324165   1.96297                             \n",
      "    72     2.291009   1.936053                            \n",
      "    73     2.299909   1.923946                            \n",
      "    74     2.291945   1.902687                            \n",
      "    75     2.261965   1.912188                            \n",
      "    76     2.256292   1.862378                            \n",
      "    77     2.283764   1.863265                            \n",
      "    78     2.269549   1.863054                            \n",
      "    79     2.215713   1.819012                            \n",
      "    80     2.233962   1.82227                             \n",
      "    81     2.200576   1.787768                            \n",
      "    82     2.162732   1.758939                            \n",
      "    83     2.142635   1.741564                            \n",
      "    84     2.135531   1.735319                            \n",
      "    85     2.176133   1.719463                            \n",
      "    86     2.160466   1.704632                            \n",
      "    87     2.141783   1.693309                            \n",
      "    88     2.088127   1.658774                            \n",
      "    89     2.111229   1.662872                            \n",
      "    90     2.085144   1.631161                            \n",
      "    91     2.080381   1.623096                            \n",
      "    92     2.053679   1.603404                            \n",
      "    93     2.018347   1.590171                            \n",
      "    94     2.030775   1.587137                            \n",
      "    95     2.005274   1.572369                            \n",
      "    96     2.041485   1.564362                            \n",
      "    97     2.040374   1.551676                            \n",
      "    98     1.990522   1.542156                            \n",
      "    99     1.972165   1.523384                            \n",
      "   100     1.970186   1.518419                            \n",
      "   101     1.941169   1.507754                            \n",
      "   102     1.930153   1.497743                            \n",
      "   103     1.992531   1.497613                            \n",
      "   104     1.980601   1.483188                            \n",
      "   105     1.937992   1.478858                            \n",
      "   106     1.953139   1.463683                            \n",
      "   107     1.943986   1.463962                            \n",
      "   108     1.920314   1.457286                            \n",
      "   109     1.919637   1.448335                            \n",
      "   110     1.899794   1.449066                            \n",
      "   111     1.928352   1.441076                            \n",
      "   112     1.919187   1.477153                            \n",
      "   113     1.894245   1.431894                            \n",
      "   114     1.919565   1.43452                             \n",
      "   115     1.919297   1.426499                            \n",
      "   116     1.89875    1.420065                            \n",
      "   117     1.913719   1.423139                            \n",
      "   118     1.906001   1.418167                            \n",
      "   119     1.883132   1.415869                            \n",
      "   120     1.879156   1.415641                            \n",
      "   121     1.978977   1.449522                            \n",
      "   122     1.885758   1.419576                            \n",
      "   123     1.922712   1.418124                            \n",
      "   124     1.925214   1.414373                            \n",
      "   125     1.905924   1.415409                            \n",
      "   126     1.954455   1.500686                            \n",
      "   127     1.972647   1.523395                            \n",
      "   128     1.978735   1.484369                            \n",
      "   129     1.967738   1.487676                            \n",
      "   130     1.947755   1.467182                            \n",
      "   131     1.933358   1.470256                            \n",
      "   132     1.935156   1.451187                            \n",
      "   133     1.906446   1.444705                            \n",
      "   134     1.91115    1.427373                            \n",
      "   135     1.923494   1.418561                            \n",
      "   136     1.924418   1.429348                            \n",
      "   137     1.984432   1.415222                            \n",
      "   138     1.899252   1.369292                            \n",
      "   139     1.897105   1.364256                            \n",
      "   140     1.841137   1.340441                            \n",
      "   141     1.84326    1.329097                            \n",
      "   142     1.868416   1.343942                            \n",
      "   143     1.840042   1.312965                            \n",
      "   144     1.874324   1.319287                            \n",
      "   145     1.811396   1.282052                            \n",
      "   146     1.762577   1.279383                            \n",
      "   147     1.888861   1.319795                            \n",
      "   148     1.845993   1.251248                            \n",
      "   149     1.815043   1.234348                            \n",
      "   150     1.762038   1.223986                            \n",
      "   151     1.786494   1.218828                            \n",
      "   152     1.720157   1.192243                            \n",
      "   153     1.787934   1.220648                            \n",
      "   154     1.75183    1.180705                            \n",
      "   155     1.744498   1.174                               \n",
      "   156     1.731648   1.173573                            \n",
      "   157     1.762455   1.158542                            \n",
      "   158     1.679742   1.125827                            \n",
      "   159     1.720433   1.133618                            \n",
      "   160     1.70213    1.116479                            \n",
      "   161     1.701468   1.12517                             \n",
      "   162     1.688673   1.095345                            \n",
      "   163     1.652056   1.121591                            \n",
      "   164     1.643764   1.074865                            \n",
      "   165     1.625612   1.068959                            \n",
      "   166     1.617944   1.054106                            \n",
      "   167     1.645849   1.053034                            \n",
      "   168     1.63424    1.041655                            \n",
      "   169     1.618739   1.036552                            \n",
      "   170     1.612805   1.028574                            \n",
      "   171     1.581608   1.015462                            \n",
      "   172     1.605815   1.016591                            \n",
      "   173     1.596237   1.003653                            \n",
      "   174     1.541984   0.98932                             \n",
      "   175     1.632894   1.009596                            \n",
      "   176     1.574649   1.021223                            \n",
      "   177     1.575777   0.969817                            \n",
      "   178     1.519004   0.962889                            \n",
      "   179     1.517987   0.953315                            \n",
      "   180     1.487708   0.942821                            \n",
      "   181     1.492317   0.935138                            \n",
      "   182     1.511452   0.934501                            \n",
      "   183     1.499541   0.923356                            \n",
      "   184     1.503385   0.919388                            \n",
      "   185     1.484187   0.913717                            \n",
      "   186     1.506033   0.908104                            \n",
      "   187     1.525332   0.899615                            \n",
      "   188     1.444354   0.89501                             \n",
      "   189     1.482232   0.889232                            \n",
      "   190     1.420826   0.887678                            \n",
      "   191     1.515024   0.932848                            \n",
      "   192     1.479149   0.875693                            \n",
      "   193     1.440456   0.865519                            \n",
      "   194     1.442502   0.860537                            \n",
      "   195     1.413896   0.854404                            \n",
      "   196     1.418872   0.859422                            \n",
      "   197     1.39866    0.844435                            \n",
      "   198     1.386183   0.852448                            \n",
      "   199     1.386025   0.842061                            \n",
      "   200     1.375029   0.827854                            \n",
      "   201     1.380948   0.823362                            \n",
      "   202     1.380488   0.815938                            \n",
      "   203     1.388114   0.815484                            \n",
      "   204     1.417501   0.812744                            \n",
      "   205     1.411235   0.863785                            \n",
      "   206     1.360058   0.832913                            \n",
      "   207     1.380308   0.801423                            \n",
      "   208     1.427368   0.795702                            \n",
      "   209     1.461475   0.798738                            \n",
      "   210     1.376885   0.792933                            \n",
      "   211     1.468042   0.783785                            \n",
      "   212     1.378427   0.783799                            \n",
      "   213     1.365334   0.780868                            \n",
      "   214     1.365856   0.777215                            \n",
      "   215     1.390694   0.770811                            \n",
      "   216     1.396444   0.769284                            \n",
      "   217     1.357945   0.755039                            \n",
      "   218     1.326287   0.761985                            \n",
      "   219     1.343257   0.762511                            \n",
      "   220     1.313105   0.750962                            \n",
      "   221     1.335734   0.758219                            \n",
      "   222     1.385638   0.755029                            \n",
      "   223     1.355301   0.751873                            \n",
      "   224     1.300687   0.749268                            \n",
      "   225     1.306017   0.744764                            \n",
      "   226     1.329351   0.750298                            \n",
      "   227     1.345246   0.749563                            \n",
      "   228     1.284247   0.736113                            \n",
      "   229     1.291705   0.739006                            \n",
      "   230     1.344582   0.734016                            \n",
      "   231     1.347407   0.741215                            \n",
      "   232     1.34059    0.727217                            \n",
      "   233     1.281131   0.732758                            \n",
      "   234     1.312791   0.729441                            \n",
      "   235     1.316706   0.733453                            \n",
      "   236     1.327567   0.72934                             \n",
      "   237     1.28997    0.730056                            \n",
      "   238     1.299706   0.726264                            \n",
      "   239     1.323021   0.72755                             \n",
      "   240     1.285228   0.724486                            \n",
      "   241     1.320296   0.726573                            \n",
      "   242     1.284853   0.729186                            \n",
      "   243     1.289675   0.729003                            \n",
      "   244     1.305194   0.766297                            \n",
      "   245     1.274874   0.720942                            \n",
      "   246     1.384209   0.730138                            \n",
      "   247     1.30372    0.728836                            \n",
      "   248     1.26465    0.731391                            \n",
      "   249     1.344122   0.786434                            \n",
      "   250     1.283259   0.723525                            \n",
      "   251     1.261634   0.721695                            \n",
      "   252     1.310802   0.727618                            \n",
      "   253     1.276433   0.72904                             \n",
      "   254     1.359733   0.795498                            \n",
      "   255     1.438137   0.809208                            \n",
      "   256     1.449979   0.800472                            \n",
      "   257     1.41651    0.813256                            \n",
      "   258     1.407325   0.8101                              \n",
      "   259     1.467972   0.819289                            \n",
      "   260     1.432339   0.801963                            \n",
      "   261     1.417025   0.797846                            \n",
      "   262     1.390911   0.790278                            \n",
      "   263     1.408597   0.832365                            \n",
      "   264     1.413694   0.824024                            \n",
      "   265     1.39518    0.822305                            \n",
      "   266     1.458935   0.793118                            \n",
      "   267     1.402918   0.779671                            \n",
      "   268     1.377309   0.774968                            \n",
      "   269     1.38849    0.765978                            \n",
      " 27%|██▋       | 16/59 [00:02<00:07,  5.97it/s, loss=1.35]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-932559647e5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle_mult\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/notebooks/volume/fastai/learner.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, lrs, n_cycle, wds, **kwargs)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0mlayer_opt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_layer_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_gen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_cycle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwarm_up\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/notebooks/volume/fastai/learner.py\u001b[0m in \u001b[0;36mfit_gen\u001b[0;34m(self, model, data, layer_opt, n_cycle, cycle_len, cycle_mult, cycle_save_name, best_save_name, use_clr, metrics, callbacks, use_wd_sched, norm_wds, wds_sched_mult, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0mn_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum_geom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcycle_len\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcycle_len\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle_mult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_cycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         return fit(model, data, n_epoch, layer_opt.opt, self.crit,\n\u001b[0;32m--> 172\u001b[0;31m             metrics=metrics, callbacks=callbacks, reg_fn=self.reg_fn, clip=self.clip, **kwargs)\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_layer_groups\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_layer_groups\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/notebooks/volume/fastai/model.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(model, data, epochs, opt, crit, metrics, callbacks, stepper, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0mbatch_num\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstepper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m             \u001b[0mavg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mavg_loss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mavg_mom\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mavg_mom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0mdebias_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mavg_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mavg_mom\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbatch_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/notebooks/volume/fastai/model.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, xs, y, epoch)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreg_fn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreg_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxtra\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m:\u001b[0m   \u001b[0;31m# Gradient clipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainable_params_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/fastai/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/fastai/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 99\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learner.fit(1e-3, 10, wds=1e-6, cycle_len=2, cycle_mult=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.save_encoder('hem_enc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.load_encoder('hem_enc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_tok = spacy.load('en')\n",
    "# TEXT = data.Field(lower=True, tokenize='spacy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "m=learner.model\n",
    "ss=\"\"\"supper I would go and see Catherine Barkley.\\nI wished she were here now. I wished I were in Milan with her  \"\"\"\n",
    "s = [spacy_tok(ss)]\n",
    "t=TEXT.numericalize(s)\n",
    "# ' '.join(s[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def brave_test(ss, words_out):\n",
    "    m=learner.model\n",
    "    s = [spacy_tok(ss)]\n",
    "    t=TEXT.numericalize(s)\n",
    "    \n",
    "    m[0].bs=1\n",
    "    m.eval()\n",
    "    m.reset()\n",
    "    res,*_ = m(t)\n",
    "    m[0].bs=bs\n",
    "    \n",
    "    nexts = torch.topk(res[-1], 10)[1]\n",
    "    [TEXT.vocab.itos[o] for o in to_np(nexts)]\n",
    "    \n",
    "    print(ss,\"\\n\")\n",
    "    for i in range(words_out):\n",
    "        n=res[-1].topk(2)[1]\n",
    "        n = n[1] if n.data[0]==0 else n[0]\n",
    "        print(TEXT.vocab.itos[n.data[0]], end=' ')\n",
    "        res,*_ = m(n[0].unsqueeze(0))\n",
    "    print('...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".  Model how are you.  \n",
      "\n",
      ". ” “ i do n’t know . ” “ i do n’t know . ...\n"
     ]
    }
   ],
   "source": [
    "brave_test(\"\"\".  Model how are you. \"\"\", 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "find: '{TRN}': No such file or directory\r\n",
      "0\r\n"
     ]
    }
   ],
   "source": [
    "!find {PATH} -name '*.txt' | xargs cat | wc -w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
